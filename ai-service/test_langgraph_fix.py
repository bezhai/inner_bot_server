#!/usr/bin/env python3
"""
æµ‹è¯•LangGraphä¿®å¤æ•ˆæœ
"""

import asyncio
import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'app'))

from app.services.chat.langgraph_chat_service import LangGraphChatService

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


async def test_langgraph_basic():
    """æµ‹è¯•LangGraphåŸºæœ¬åŠŸèƒ½"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•LangGraphåŸºæœ¬åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯ID
        test_message_id = "test_msg_001"
        
        # æµ‹è¯•LangGraphèŠå¤©æœåŠ¡
        chat_service = LangGraphChatService()
        
        # è°ƒç”¨æµå¼AIå›å¤
        response_count = 0
        async for chunk in chat_service.stream_ai_reply(
            message_id=test_message_id,
            model_id="gpt-4o-mini",
            temperature=0.7,
            enable_tools=False,  # å…ˆç¦ç”¨å·¥å…·æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            yield_interval=0.1
        ):
            response_count += 1
            if chunk.content:
                logger.info(f"æ”¶åˆ°å“åº”å— {response_count}: {chunk.content[:50]}...")
            elif chunk.reason_content:
                logger.info(f"æ”¶åˆ°åŸå› å†…å®¹ {response_count}: {chunk.reason_content[:50]}...")
            elif chunk.tool_call_feedback:
                logger.info(f"æ”¶åˆ°å·¥å…·åé¦ˆ {response_count}: {chunk.tool_call_feedback}")
            
            # é™åˆ¶æµ‹è¯•å“åº”æ•°é‡
            if response_count >= 5:
                logger.info("è¾¾åˆ°æµ‹è¯•å“åº”é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        logger.info(f"LangGraphåŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆï¼Œå…±æ”¶åˆ° {response_count} ä¸ªå“åº”å—")
        return True
        
    except Exception as e:
        logger.error(f"LangGraphåŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_langgraph_with_tools():
    """æµ‹è¯•LangGraphå·¥å…·è°ƒç”¨åŠŸèƒ½"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•LangGraphå·¥å…·è°ƒç”¨åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯ID
        test_message_id = "test_msg_002"
        
        # æµ‹è¯•LangGraphèŠå¤©æœåŠ¡
        chat_service = LangGraphChatService()
        
        # è°ƒç”¨æµå¼AIå›å¤ï¼ˆå¯ç”¨å·¥å…·ï¼‰
        response_count = 0
        tool_calls_detected = False
        async for chunk in chat_service.stream_ai_reply(
            message_id=test_message_id,
            model_id="gpt-4o-mini",
            temperature=0.7,
            enable_tools=True,  # å¯ç”¨å·¥å…·æµ‹è¯•
            yield_interval=0.1
        ):
            response_count += 1
            if chunk.content:
                logger.info(f"æ”¶åˆ°å“åº”å— {response_count}: {chunk.content[:50]}...")
            elif chunk.reason_content:
                logger.info(f"æ”¶åˆ°åŸå› å†…å®¹ {response_count}: {chunk.reason_content[:50]}...")
            elif chunk.tool_call_feedback:
                logger.info(f"æ”¶åˆ°å·¥å…·åé¦ˆ {response_count}: {chunk.tool_call_feedback}")
                tool_calls_detected = True
            
            # é™åˆ¶æµ‹è¯•å“åº”æ•°é‡
            if response_count >= 15:
                logger.info("è¾¾åˆ°æµ‹è¯•å“åº”é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        logger.info(f"LangGraphå·¥å…·è°ƒç”¨åŠŸèƒ½æµ‹è¯•å®Œæˆï¼Œå…±æ”¶åˆ° {response_count} ä¸ªå“åº”å—")
        logger.info(f"å·¥å…·è°ƒç”¨æ£€æµ‹: {'âœ“ æœ‰å·¥å…·è°ƒç”¨' if tool_calls_detected else 'âœ— æ— å·¥å…·è°ƒç”¨'}")
        return True
        
    except Exception as e:
        logger.error(f"LangGraphå·¥å…·è°ƒç”¨åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_langgraph_stability():
    """æµ‹è¯•LangGraphç¨³å®šæ€§ï¼ˆæ— æ•°æ®åº“ä¾èµ–ï¼‰"""
    try:
        logger.info("å¼€å§‹æµ‹è¯•LangGraphç¨³å®šæ€§...")
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯ID
        test_message_id = "test_stability_msg"
        
        # æµ‹è¯•LangGraphèŠå¤©æœåŠ¡
        chat_service = LangGraphChatService()
        
        # è°ƒç”¨æµå¼AIå›å¤ï¼ˆå¯ç”¨å·¥å…·ï¼‰
        response_count = 0
        no_crash_detected = True
        
        async for chunk in chat_service.stream_ai_reply(
            message_id=test_message_id,
            model_id="gpt-4o-mini",
            temperature=0.7,
            enable_tools=True,  # å¯ç”¨å·¥å…·æµ‹è¯•
            yield_interval=0.1
        ):
            response_count += 1
            if chunk.content:
                content = chunk.content
                logger.info(f"æ”¶åˆ°å“åº”å— {response_count}: {content[:50]}...")
            elif chunk.reason_content:
                logger.info(f"æ”¶åˆ°åŸå› å†…å®¹ {response_count}: {chunk.reason_content[:50]}...")
            elif chunk.tool_call_feedback:
                logger.info(f"ğŸ”§ æ”¶åˆ°å·¥å…·åé¦ˆ {response_count}: {chunk.tool_call_feedback}")
            
            # é™åˆ¶æµ‹è¯•å“åº”æ•°é‡
            if response_count >= 8:
                logger.info("è¾¾åˆ°æµ‹è¯•å“åº”é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                break
        
        logger.info(f"LangGraphç¨³å®šæ€§æµ‹è¯•å®Œæˆï¼Œå…±æ”¶åˆ° {response_count} ä¸ªå“åº”å—")
        logger.info(f"ç¨³å®šæ€§æ£€æµ‹: {'âœ“ æ— å´©æºƒ' if no_crash_detected else 'âœ— æœ‰å´©æºƒ'}")
        
        # åªè¦æ²¡æœ‰å´©æºƒå°±ç®—æˆåŠŸ
        return True
        
    except Exception as e:
        logger.error(f"LangGraphç¨³å®šæ€§æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹LangGraphä¿®å¤éªŒè¯æµ‹è¯•...")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    basic_test_result = await test_langgraph_basic()
    
    # æµ‹è¯•å·¥å…·è°ƒç”¨åŠŸèƒ½
    tools_test_result = await test_langgraph_with_tools()
    
    # æµ‹è¯•LangGraphç¨³å®šæ€§
    stability_test_result = await test_langgraph_stability()
    
    # è¾“å‡ºæµ‹è¯•ç»“æœ
    logger.info("=" * 50)
    logger.info("æµ‹è¯•ç»“æœæ±‡æ€»:")
    logger.info(f"åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {'âœ“ é€šè¿‡' if basic_test_result else 'âœ— å¤±è´¥'}")
    logger.info(f"å·¥å…·è°ƒç”¨æµ‹è¯•: {'âœ“ é€šè¿‡' if tools_test_result else 'âœ— å¤±è´¥'}")
    logger.info(f"ç¨³å®šæ€§æµ‹è¯•: {'âœ“ é€šè¿‡' if stability_test_result else 'âœ— å¤±è´¥'}")
    
    if basic_test_result and tools_test_result and stability_test_result:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LangGraphä¿®å¤æˆåŠŸï¼Œè¿è¡Œç¨³å®š")
        return 0
    else:
        logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®å¤")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
